# Data-Warehousing-with-AWS-Redshift

![rs_logo](https://github.com/eaamankwah/Data-Warehousing-with-AWS-Redshift/blob/main/screenshots/rs_logo.png)

# Table of Contents
* Overview
* Project Dataset
* Star Database Schema Design
* * Staging Tables
* * Fact Table
* * Dimension Tables
* Project Steps
* * Project Files
* * Running the Project
* * Analytics Test Query Results
* Project Component Steps
* * Create Table Schema
* * Build ETL Pipeline
* * Document Process
* References

## Overview
This project is third project of the Udacity Data Engineering Nanodegree. 

The object of this project is to construct a cloud-based data warehouse with AWS Redshift, which includes building an ETL pipeline that extracts their data from AWS S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analysis that general data driven insight for the proposed client, Sparkify music business.

## Project Dataset

The dataset is a collected by and online startup platform called Sparkify who wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Sparkify wants to access the songs  that their users are listening to. The  data currently S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

**Song Dataset**
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

For example,  below are file paths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json 
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

**Log Dataset**
The second dataset consists of log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month. For example, here are file paths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json 
log_data/2018/11/2018-11-13-events.json

## Star Database Schema Design

A star schema was used for this project and has the following benefits:

* Queries are simpler: because all of the data connects through the fact table the multiple dimension tables are treated as one large table of information, and that makes queries simpler and easier to perform.
* Easier business insights reporting: Star schemas simplify the process of pulling business reports like "what songs users are listening to".
* Better-performing queries: by removing the bottlenecks of a highly normalized schema, query speed increases, and the performance of read-only commands improves.
* Provides data to OLAP systems: OLAP (Online Analytical Processing) systems can use star schemas to build OLAP cubes.
* Since the data set is small and structured, the star schema is sufficient to model using ERD models and can easily be queried using SQL joins

The star schema optimized for queries on song play analysis includes the tables below:

### Staging Table

Staging tables are permanent tables used to store temporary data before transferring to another permanent table. This seemly intermediate process helps to process data from externa sources before transferring them to the database tables.  Some of the immediate benefits of staging tables includes data cleansing, computing values based on source data, re-shaping and/or re-distributing source data layout to one that matches the needs of a data warehouse.

Advantages of Staging Tables

* Baseline: the staging tables provides buffer that isolates the warehouse from the source systems, and when warehouse processing is interrupted and has to be restarted.
* Traceability: the data sources develop based on operational needs. The staging tables capture source data at the time of each extract and permit strict traceability from user analytics back through to source data.
* Agility: staging allows developers to use SQL to access staging data and test different warehouse structures. While simple SQL codes can be left in place, complex SQL codes added into the ETL process, thereby providing agility in the warehouse development.
* Warehouse evolution: developers can reprocess warehouse data upon restructuring and during agile development
* [Dataversity](https://www.dataversity.net/data-warehouses-stage-source-data/#:~:text=Staging%20tables%20provide%20a%20buffer,data%20from%20their%20operational%20counterparts.)

In this project, two staging tables are provided below:

#### Staging_events

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   artist    | VARCHAR      |    | 
|  auth    |  VARCHAR    |    | 
|   firstname   |   VARCHAR    |       | 
| gender    |   VARCHAR  |       | 
|   itemInSession    |  INTEGER   |       | 
|   lastName   |   VARCHAR    |       | 
|   length    |   FLOAT   |       | 
|  level    |   VARCHAR    |       | 
|   location    |   VARCHAR    |       | 
|   method    | VARCHAR      |     | 
|   page    |  VARCHAR    |   | 
|  registration    |   FLOAT    |       | 
|   sessionId    |   INTEGER  |       | 
|   song    |   VARCHAR   |      | 
|   status    |   INTEGER   |     | 
|   ts    |   TIMESTAMP    |       | 
|   userAgent    |   VARCHAR    |       | 
|   userId    |   INTEGER    |       | 


#### Staging _songs

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   num_songst    | INTEGER      |    | 
|  artist_id    |  VARCHAR    |    | 
|   artist_latitude   |   FLOAT    |       | 
| artist_longitude    |   FLOAT  |       | 
|   artist_location    |  VARCHAR   |       | 
|   artist_name   |   VARCHAR    |       | 
|   song_id    |   VARCHAR   |       | 
|  title   |   VARCHAR    |       | 
|   duration    |   FLOAT   |       | 
|   year    | INTEGER      |     | 


### Fact Table
The main fact table which contains all the measures associated with each event (user song plays) is shown below:

#### Songplays Table

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   songplay_id    | INTEGER      |  IDENTITY(0,1) PRIMARY KEY   | 
|   start_time    |  TIMESTAMP    |  NOT NULL SORTKEY DISTKEY  | 
|   user_id    |   INTEGER    |   NOT NULL    | 
|   level    |   VARCHAR  |       | 
|   song_id    |   VARCHAR   |    NOT NULL   | 
|   artist_id    |   VARCHAR    |    NOT NULL   | 
|   session_id    |   INTEGER    |       | 
|   location    |   VARCHAR    |       | 
|   user_agent    |   VARCHAR    |       | 

The songplay_id field has the primary key constraint and it is an auto-incremental value.


### Dimension Tables

The dimension tables below contain detailed information about each row in the fact table.

#### User Table

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   user_id    | INTEGER      |  NOT NULL SORTKEY PRIMARY KEY    | 
|   first_name    |    VARCHAR    |    NOT NULL  | 
|   last_name    |    VARCHAR    |   NOT NULL   | 
|   gender    |    VARCHAR | NOT NULL | 
|   level    |    VARCHAR    | NOT NULL | 

#### Songs Table

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   song_id    |     VARCHAR     |  NOT NULL SORTKEY PRIMARY KEY    | 
|   title    |    VARCHAR    |   NOT NULL   | 
|   artist_id    |    VARCHAR    |   NOT NULL   | 
|   year    |   INTEGER | NOT NULL | 
|   duration    |   FLOAT    |       | 

#### Artists Table 

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   artist_id    |    VARCHAR     |   NOT NULL SORTKEY PRIMARY KEY    | 
|   name    |   VARCHAR    |    NOT NULL   | 
|   location    |   VARCHAR    |       | 
|   latitude    |   FLOAT    |       | 
|   longitude    |   FLOAT   |       | 

#### Time Table 

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   start_time    |  TIMESTAMP      |   NOT NULL DISTKEY SORTKEY PRIMARY KEY   | 
|   hour    |  INTEGER    | NOT NULL | 
|   day    |   INTEGER    | NOT NULL | 
|   week    |   INTEGER   | NOT NULL | 
|   month    |   INTEGER    | NOT NULL | 
|   year    |   INTEGER    | NOT NULL | 
|   weekday    |   VARCHAR(20)    | NOT NULL | 


## Project Steps

### Project Files

Below are the files related to the completion of the project:

1. data - S3 directory contains the data files, including log and song datasets.
2. dwh.cfg - configuration file containg redshift database and IAM role information
3. test_analytics.py  - python script to display the number of rows in each table, indicating the proper functioning of the database.
3. create_tables.py - python script to drop and create  tables in order to reset tables before each time the ETL scripts are run.
4. creat_cluster.ipynb -  this notebook contains instructions and codes to set up Redshift infrastructure for data warehousing.
5. etl.py - reads and processes files from song_data and log_data and loads them into tables. 
6. sql_queries.py - contains all the queries, and is imported into the test_analytics.py, creat_tables.py and etl.py files.
7. README.md - provides discussion and summary on this project.

### Running the project

1. The following configuration information were provided and saved in the file *dwh.cfg*.

```
[CLUSTER]
HOST=dwhcluster.xxxxxxxxxxxx.us-west-2.redshift.amazonaws.com
DB_NAME=''
DB_USER=''
DB_PASSWORD=''
DB_PORT=5439

[IAM_ROLE]
ARN=arn:aws:iam::xxxxxxx:role/dwhRole

[S3]
LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'

[AWS]
KEY=
SECRET=

[DWH]
DWH_CLUSTER_TYPE       = multi-node
DWH_NUM_NODES          = 4
DWH_NODE_TYPE          = dc2.large
DWH_CLUSTER_IDENTIFIER = 
DWH_DB                 = 
DWH_DB_USER            = 
DWH_DB_PASSWORD        = 
DWH_PORT               = 5439
DWH_IAM_ROLE_NAME      = 
```
2. The infrastructure as code (IaC) technique was followed in the **create_cluster.ipynb** notebook to set up the needed cluster and database for this project.

4. A terminal was opened and the **create_tables.py** script was run to set up the database staging and analytical tables.

`$ python create_tables.py`

5. The  **etl.py** script was run to extract data from the files in S3, stage it in redshift, and then store in the dimensional tables.

`$ python etl.py`

6. The **test_analytic.py** script was finally run to provide example queries and results, indicating the functionality of the database.

`$ python test_analytics.py`

### Analytic Test Query Results

The following query was run to test the functionality of the database:

' analytics_queries= [get_number_staging_events, get_number_staging_songs, get_number_songplays, get_number_users, get_number_songs, get_number_artists, get_number_time]Number of rows in each table:'

Below is the result of the above query:

| Table            | rows  |
|---               | --:   |
| staging_events   | 8056  |
| staging_songs    | 14896 |
| songplays        | 333   |
| users            |  104  |
| songs            | 14896 |
| artists          | 10025 |
| time             |  8023 |

**Note** all the resources were deleted after the analytic testing.

## Project Component Steps

### Create Table Schema

1. Design schemas for your fact and dimension tables
2. Write a SQL CREATE statement for each of these tables in sql_queries.py
3. Complete the logic in create_tables.py to connect to the database and create these tables
4. Write SQL DROP statements to drop tables in the beginning of create_tables.py if the tables already exist. This way, you can run create_tables.py whenever you want to reset your database and test your ETL pipeline.
5. Launch a redshift cluster and create an IAM role that has read access to S3.
6. Add redshift database and IAM role info to dwh.cfg.
7. Test by running create_tables.py and checking the table schemas in your redshift database. You can use Query Editor in the AWS Redshift console for this.

### Build ETL Pipeline

1. Implement the logic in etl.py to load data from S3 to staging tables on Redshift.
2. Implement the logic in etl.py to load data from staging tables to analytics tables on Redshift.
3. Test by running etl.py after running create_tables.py and running the analytic queries on your Redshift database to compare your results with the expected results.
4. Delete your redshift cluster when finished.

### Document Process

Do the following steps in your README.md file.
1. Discuss the purpose of this database in context of the startup, Sparkify, and their analytical goals.
2. State and justify your database schema design and ETL pipeline.
3. Provide example queries and results for song play analysis.

## References

* [Dataversity](https://www.dataversity.net/data-warehouses-stage-source-data/#:~:text=Staging%20tables%20provide%20a%20buffer,data%20from%20their%20operational%20counterparts.)

* [Udacity Q & A Platform](https://knowledge.udacity.com/?nanodegree=nd027&page=1&project=574&rubric=2501)


